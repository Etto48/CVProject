{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision and Language Models project\n",
    "\n",
    "Group: The Karate Kid\n",
    "\n",
    "- [Ettore Ricci](https://github.com/Etto48)\n",
    "- [Paolo Palumbo](https://github.com/paolpal)\n",
    "- [Zahra Omrani](https://github.com/zahra-omrani)\n",
    "- [Erni Delialisi](https://github.com/erni-de)\n",
    "\n",
    "The whole codebase can be found on [GitHub](https://github.com/Etto48/CVProject).\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The goal of this project is to build a model that can generate a textual description of an image.\n",
    "We used the [COCO dataset](https://cocodataset.org/#home).\n",
    "Our model is a combination of a pre-trained ViT and a GPT-like model trained from scratch.\n",
    "The overall architecture is inspired by GIT (Generative Image Transformer) from Microsoft.\n",
    "\n",
    "The pre-trained ViT is [DINOv2](https://arxiv.org/abs/2304.07193).\n",
    "\n",
    "GIT paper can be found [here](https://arxiv.org/abs/2205.14100)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch tiktoken transformers pandas torchvision numpy matplotlib scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal, Optional, Callable\n",
    "import torch\n",
    "import requests\n",
    "from torch import nn\n",
    "import tiktoken\n",
    "from tqdm.auto import tqdm\n",
    "import transformers\n",
    "import json\n",
    "import os\n",
    "import zipfile\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import patches\n",
    "import sklearn\n",
    "import sklearn.decomposition\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "The dataset is the COCO dataset, which contains images and (among other things) their textual descriptions.\n",
    "We used the 2017 version of the dataset, which can be downloaded from the [official website](https://cocodataset.org/#download)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextImageDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        assert 'caption' in df.columns\n",
    "        assert 'image_path' in df.columns\n",
    "        self.df = df\n",
    "        self.tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def load(captions_path: str, images_path: str):\n",
    "        img_name_padding = 12\n",
    "        \n",
    "        with open(captions_path, \"r\") as f:\n",
    "            captions_data = json.load(f)[\"annotations\"]\n",
    "        df_list = []\n",
    "        for caption_data in tqdm(captions_data, desc=\"Loading dataset\"):\n",
    "            image_id = caption_data[\"image_id\"]\n",
    "            caption = caption_data[\"caption\"]\n",
    "            image_name = f\"{str(image_id).zfill(img_name_padding)}.jpg\"\n",
    "            image_path = os.path.join(images_path, image_name)\n",
    "            df_list.append({\"caption\": caption, \"image_path\": image_path})\n",
    "        df = pd.DataFrame.from_dict(df_list)\n",
    "        return TextImageDataset(df)\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_train():\n",
    "        return TextImageDataset.load(os.path.expanduser(\"~/Downloads/COCO/annotations_trainval2017/annotations/captions_train2017.json\"), os.path.expanduser(\"~/Downloads/COCO/train2017\"))\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_valid():\n",
    "        return TextImageDataset.load(os.path.expanduser(\"~/Downloads/COCO/annotations_trainval2017/annotations/captions_val2017.json\"), os.path.expanduser(\"~/Downloads/COCO/val2017\"))\n",
    "    \n",
    "    @staticmethod\n",
    "    def download(valid: bool = False):\n",
    "        if valid:\n",
    "            imgs = \"http://images.cocodataset.org/zips/val2017.zip\"\n",
    "        else:\n",
    "            imgs = \"http://images.cocodataset.org/zips/train2017.zip\"\n",
    "        annotations = \"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\"\n",
    "        coco_path = os.path.expanduser(\"~/Downloads/COCO\")\n",
    "        os.makedirs(coco_path, exist_ok=True)\n",
    "\n",
    "        # Downloading imgs2017.zip\n",
    "        imgs_path = f\"{coco_path}/train2017.zip\" if not valid else f\"{coco_path}/val2017.zip\"\n",
    "        if not os.path.exists(imgs_path):\n",
    "            val_imgs_stream = requests.get(imgs, stream=True)\n",
    "            val_size = int(val_imgs_stream.headers.get(\"Content-Length\", 0))\n",
    "            with open(imgs_path, \"wb\") as f:\n",
    "                with tqdm(\n",
    "                    total=val_size, \n",
    "                    unit=\"B\", \n",
    "                    unit_scale=True, \n",
    "                    desc=\"Downloading images\") as bar:\n",
    "                    for chunk in val_imgs_stream.iter_content(chunk_size=4096):\n",
    "                        f.write(chunk)\n",
    "                        bar.update(len(chunk))\n",
    "        \n",
    "        # Downloading annotations_trainval2017.zip\n",
    "        if not os.path.exists(f\"{coco_path}/annotations_trainval2017.zip\"):\n",
    "            annotations_stream = requests.get(annotations, stream=True)\n",
    "            annotations_size = int(annotations_stream.headers.get(\"Content-Length\", 0))\n",
    "            with open(f\"{coco_path}/annotations_trainval2017.zip\", \"wb\") as f:\n",
    "                with tqdm(\n",
    "                    total=annotations_size, \n",
    "                    unit=\"B\", \n",
    "                    unit_scale=True, \n",
    "                    desc=\"Downloading annotations\") as bar:\n",
    "                    for chunk in annotations_stream.iter_content(chunk_size=4096):\n",
    "                        f.write(chunk)\n",
    "                        bar.update(len(chunk))\n",
    "        \n",
    "        # Extracting val2017.zip\n",
    "        imgs_path = imgs_path[:-4] # remove .zip\n",
    "        if not os.path.exists(imgs_path):\n",
    "            with zipfile.ZipFile(f\"{imgs_path}.zip\", \"r\") as zip_ref:\n",
    "                zip_ref.extractall(coco_path)\n",
    "\n",
    "        # Extracting annotations_trainval2017.zip\n",
    "        os.makedirs(f\"{coco_path}/annotations_trainval2017\", exist_ok=True)\n",
    "        if not os.path.exists(f\"{coco_path}/annotations_trainval2017/annotations\"):\n",
    "            with zipfile.ZipFile(f\"{coco_path}/annotations_trainval2017.zip\", \"r\") as zip_ref:\n",
    "                zip_ref.extractall(f\"{coco_path}/annotations_trainval2017\")\n",
    "\n",
    "        if valid:\n",
    "            return TextImageDataset.load_valid()\n",
    "        else:\n",
    "            return TextImageDataset.load_train()\n",
    "    \n",
    "    def __len__(self):  \n",
    "        return len(self.df)\n",
    "\n",
    "    def _load_image(self, image_path: str):\n",
    "        image = torchvision.io.decode_image(torchvision.io.read_file(image_path))\n",
    "        if image.shape[0] == 1:\n",
    "            image = image.repeat(3, 1, 1)\n",
    "        return image\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        caption = row[\"caption\"]\n",
    "        plaintext_caption = caption\n",
    "        image_path = row[\"image_path\"]\n",
    "        image = self._load_image(image_path)\n",
    "        caption = [self.tokenizer.max_token_value] + self.tokenizer.encode(caption) + [self.tokenizer.max_token_value]\n",
    "        caption = torch.tensor(caption, dtype=torch.long)\n",
    "        return image, caption, plaintext_caption\n",
    "    \n",
    "    @staticmethod\n",
    "    def collate_fn(batch: list):\n",
    "        images, captions, plaintext_caption = zip(*batch)\n",
    "        lengths = torch.tensor([len(c) for c in captions])\n",
    "        captions = torch.nn.utils.rnn.pad_sequence(captions, batch_first=True, padding_value=0)\n",
    "        return images, captions, lengths, plaintext_caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = TextImageDataset.load_train()\n",
    "valid = TextImageDataset.download(valid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 3\n",
    "w = 1\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    dataset=valid, batch_size=h*w, collate_fn=TextImageDataset.collate_fn,\n",
    "    sampler=torch.utils.data.RandomSampler(valid, replacement=True, num_samples=h * w))\n",
    "images, _, _, captions = next(iter(valid_loader))\n",
    "for i in range(h*w):\n",
    "    plt.subplot(h, w, i + 1)\n",
    "    plt.imshow(images[i].permute(1, 2, 0))\n",
    "    plt.title(captions[i])\n",
    "    plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DINOv2\n",
    "\n",
    "We used a pre-trained ViT model, DINOv2. It was trained with a self-supervised learning approach.\n",
    "During the training, two models are trained: a teacher and a student.\n",
    "\n",
    "The teacher model is used to generate the pseudo-labels for the student model.\n",
    "The student model is trained to predict the pseudo-labels generated by the teacher model.\n",
    "\n",
    "The resulting model is capable of extracting meaningful and robust features from the input images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_processor = transformers.AutoImageProcessor.from_pretrained('facebook/dinov2-large')\n",
    "img_embedding = transformers.AutoModel.from_pretrained('facebook/dinov2-large').to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will display an example of the PCA of the features for some images. The features are relative to their respective patch of the image.\n",
    "\n",
    "There are 3 plots for each image:\n",
    "- The first shows the image.\n",
    "- The second shows the PCA relative to all the images. Here we can see that similar objects in different images are colored similarly.\n",
    "- The third shows the PCA relative to the image itself. Here we can see that the different components of an object are colored differently while similar objects are still colored similarly.\n",
    "\n",
    "These plots show that DINOv2 is capable of extracting both global and local features from the images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_embedding_to_pca(image_embedding: torch.Tensor):\n",
    "    if len(image_embedding.shape) == 2:\n",
    "        image_embedding = image_embedding.unsqueeze(0)\n",
    "    assert len(image_embedding.shape) == 3, f\"Expected 3D tensor of batched embeddings, got {image_embedding.shape}\"\n",
    "    batch_size = image_embedding.shape[0]\n",
    "    image_embedding = image_embedding[:, 1:, :]\n",
    "    edge_square = image_embedding.shape[1]\n",
    "    feature_size = image_embedding.shape[2]\n",
    "    edge = int(edge_square ** 0.5)\n",
    "    assert edge ** 2 == edge_square, f\"Expected square image embeddings, got {edge_square}\"\n",
    "    pca_local = [sklearn.decomposition.PCA(n_components=3)] * batch_size\n",
    "    pca = sklearn.decomposition.PCA(n_components=3)\n",
    "    pca.fit(image_embedding.reshape(batch_size * edge_square, feature_size).cpu().numpy())\n",
    "    ret_local = []\n",
    "    ret = []\n",
    "    image_embedding = image_embedding.cpu().numpy()\n",
    "    for i in range(batch_size):\n",
    "        pca_local[i].fit(image_embedding[i])\n",
    "        pca_img = pca.transform(image_embedding[i]).reshape(edge, edge, 3)\n",
    "        pca_img = (pca_img - pca_img.min()) / (pca_img.max() - pca_img.min())\n",
    "        ret.append(pca_img)\n",
    "        pca_img_local = pca_local[i].transform(image_embedding[i]).reshape(edge, edge, 3)\n",
    "        pca_img_local = (pca_img_local - pca_img_local.min()) / (pca_img_local.max() - pca_img_local.min())\n",
    "        ret_local.append(pca_img_local)\n",
    "    return ret, ret_local\n",
    "\n",
    "h = 5\n",
    "w = 3\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    dataset=valid, batch_size=h * w, collate_fn=TextImageDataset.collate_fn,\n",
    "    sampler=torch.utils.data.RandomSampler(valid, replacement=False, num_samples=h * w, generator=torch.Generator().manual_seed(4)))\n",
    "images, _, _, _ = next(iter(valid_loader))\n",
    "with torch.no_grad():\n",
    "    preprocessed_images = img_processor(images, return_tensors=\"pt\").to(device)\n",
    "    image_embeddings = img_embedding(**preprocessed_images).last_hidden_state\n",
    "pca, pca_local = image_embedding_to_pca(image_embeddings)\n",
    "fig = plt.figure(figsize=(15, 8))\n",
    "for i in range(h*w):\n",
    "    plt.subplot(h, 3*w, 3*i + 1)\n",
    "    img = preprocessed_images[\"pixel_values\"][i].permute(1, 2, 0)\n",
    "    img = (img - img.min()) / (img.max() - img.min())\n",
    "    plt.imshow(img.cpu())\n",
    "    plt.axis(\"off\")\n",
    "    begin = plt.gca().get_position()\n",
    "    plt.subplot(h, 3*w, 3*i + 2)\n",
    "    plt.imshow(pca[i])\n",
    "    plt.axis(\"off\")\n",
    "    plt.subplot(h, 3*w, 3*i + 3)\n",
    "    plt.imshow(pca_local[i])\n",
    "    plt.axis(\"off\")\n",
    "    end = plt.gca().get_position()\n",
    "    rect = patches.Rectangle((begin.x0, begin.y0), end.x1 - begin.x0, end.y1 - begin.y0, edgecolor='black', facecolor='none', lw=2, transform=fig.transFigure)\n",
    "    fig.patches.append(rect)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional encoding\n",
    "\n",
    "We added positional encoding to only the text embeddings, not the image embeddings.\n",
    "This is because the image embeddings already contain positional information, while the text embeddings do not.\n",
    "Also, the classical positional encoding is not suitable for the image embeddings, as it does not preserve rotational invariance and scale invariance.\n",
    "\n",
    "We also added dropout to the positional encoding to prevent overfitting on the positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.pe = None\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def set_pe(self, t, d, device):\n",
    "        position = torch.arange(t, device=device).view(1, -1, 1)\n",
    "        div_term = torch.exp(torch.arange(0, d, 2, device=device) * -np.log(10000.0) / d).view(1, 1, -1)\n",
    "        self.pe = torch.zeros(1, t, d, device=device)\n",
    "        self.pe[:, :, 0::2] = torch.sin(position * div_term)\n",
    "        self.pe[:, :, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        _, t, d = x.size()\n",
    "        if self.pe is None or t > self.pe.size(1) or d != self.pe.size(2) or self.pe.device != x.device:\n",
    "            self.set_pe(t, d, x.device)\n",
    "\n",
    "        x = x + self.pe[:, :t, :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we show an example of the positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe = PositionalEncoding(0)\n",
    "x = torch.zeros(1, 512, 128)\n",
    "y = pe(x)\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.title(\"Positional Encoding\")\n",
    "plt.imshow(y[0].T.detach().cpu().numpy())\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"Time-step\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "We used the GPT-2 tokenizer to tokenize the text descriptions.\n",
    "We chose this tokenizer because it has a relatively small vocabulary size (50257 tokens) and, being a Byte-Pair Encoding (BPE) tokenizer, it can handle every word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "![model architecture](https://raw.githubusercontent.com/Etto48/CVProject/refs/heads/main/docs/graph_rendered.svg)\n",
    "\n",
    "Our model takes as input the concatenation of the image embeddings (from DINOv2) and the text embeddings.\n",
    "This model is an autoregressive model, like GPT.\n",
    "\n",
    "The key difference is that we used a mask that is causal for text embeddings and non-causal for image embeddings because the image embeddings need to attend to each other while the text embeddings can only attend to the previous tokens and the image embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Annotator(nn.Module):\n",
    "    # for later\n",
    "    from_pretrained: Callable\n",
    "    encode_images: Callable\n",
    "    forward_with_embeddings: Callable\n",
    "    loss_fn: Callable\n",
    "    fit: Callable\n",
    "    save: Callable\n",
    "    load: Callable\n",
    "    _sample_search: Callable\n",
    "    _greedy_search: Callable\n",
    "    _beam_search: Callable\n",
    "    annotate: Callable\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.device = torch.device(\n",
    "            \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.embedding_dim = 1024\n",
    "        self.heads = 16\n",
    "        self.decoder_block_depth = 6\n",
    "        self.decoder_blocks = 3\n",
    "        self.dropout = 0.1\n",
    "\n",
    "        self.tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "        self.embedding = nn.Sequential(\n",
    "            nn.Embedding(\n",
    "                num_embeddings=self.tokenizer.max_token_value + 1,\n",
    "                embedding_dim=self.embedding_dim,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        self.positional_encoding = PositionalEncoding(self.dropout)\n",
    "       \n",
    "        self.img_processor = transformers.AutoImageProcessor.from_pretrained('facebook/dinov2-large')\n",
    "        self.img_embedding = transformers.AutoModel.from_pretrained('facebook/dinov2-large')\n",
    "\n",
    "        for param in self.img_embedding.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.causal = nn.ModuleList()\n",
    "        for _ in range(self.decoder_blocks):\n",
    "            self.causal.append(nn.TransformerEncoder(\n",
    "                nn.TransformerEncoderLayer(\n",
    "                    d_model=self.embedding_dim,\n",
    "                    nhead=self.heads,\n",
    "                    dim_feedforward=self.embedding_dim * 4,\n",
    "                    dropout=self.dropout,\n",
    "                    batch_first=True,\n",
    "                    activation=\"gelu\"),\n",
    "                num_layers=self.decoder_block_depth,\n",
    "                enable_nested_tensor=True,\n",
    "                norm=nn.LayerNorm(self.embedding_dim)\n",
    "            ))\n",
    "\n",
    "        self.deembedding = nn.Sequential(\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(self.embedding_dim, self.tokenizer.max_token_value + 1)\n",
    "        )\n",
    "        self.to(self.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need this function to download the weights of the model from my Google Drive so that we don't have to train it on Colab.\n",
    "Also the dataset is 20GB so it would take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@staticmethod\n",
    "def from_pretrained():\n",
    "    url = \"https://huggingface.co/Etto48/CVProject/resolve/main/data/annotator.pt\"\n",
    "    file_stream = requests.get(url, stream=True)\n",
    "    file_stream.raise_for_status()\n",
    "    file_size = int(file_stream.headers[\"Content-Length\"])\n",
    "    os.makedirs(\"data\", exist_ok=True)\n",
    "    if not os.path.exists(\"data/annotator.pt\"):\n",
    "        with open(\"data/annotator.pt\", \"wb\") as f:\n",
    "            with tqdm(total=file_size, desc=\"Downloading model weights\", unit=\"B\", unit_scale=True) as pbar:\n",
    "                for chunk in file_stream.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "                    pbar.update(len(chunk))\n",
    "    else:\n",
    "        if os.path.getsize(\"data/annotator.pt\") != file_size:\n",
    "            print(\"File size mismatch, maybe the file is an old version or a partial download. Please remove the file and try again.\")\n",
    "            return None\n",
    "    annotator = Annotator.load(\"data/annotator.pt\")\n",
    "    return annotator\n",
    "Annotator.from_pretrained = from_pretrained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function encodes the images into image embeddings using DINOv2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_images(self: Annotator, images):\n",
    "    img_inputs = self.img_processor(images, return_tensors=\"pt\").to(self.device)\n",
    "    image_embeddings = self.img_embedding(**img_inputs).last_hidden_state\n",
    "    return image_embeddings\n",
    "Annotator.encode_images = encode_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes as input the image embeddings computed by DINOv2 and the tokenized captions (generated up to the current token).\n",
    "Every caption is prepended with a special token so that it's never empty.\n",
    "\n",
    "This function then computes the embedding of the caption, adding positional encoding, concatenates it with the image embeddings, and passes it through the model together with the mask described above.\n",
    "\n",
    "Then the image embeddings are discarded and the text embeddings are used to predict the next token probability distribution for each token in the caption using a linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_with_embeddings(self: Annotator, generated_caption, image_embeddings):\n",
    "    caption_embeddings: torch.Tensor = self.embedding(generated_caption)\n",
    "\n",
    "    tgt_mask = nn.Transformer.generate_square_subsequent_mask(\n",
    "        image_embeddings.shape[1] + caption_embeddings.shape[1], device=self.device)\n",
    "    tgt_mask[:image_embeddings.shape[1], :image_embeddings.shape[1]] = 0\n",
    "    caption_embeddings = self.positional_encoding(caption_embeddings)\n",
    "    input_sequence = torch.cat([image_embeddings, caption_embeddings], dim=1)\n",
    "    output_sequence = input_sequence\n",
    "\n",
    "    for causal in self.causal:\n",
    "        output_sequence = causal(output_sequence, mask=tgt_mask) + output_sequence\n",
    "        output_sequence = nn.functional.layer_norm(output_sequence, output_sequence.shape[1:])\n",
    "    \n",
    "    output_sequence = output_sequence[:, image_embeddings.shape[1]:, :]\n",
    "    output_sequence = self.deembedding(output_sequence)\n",
    "    return output_sequence\n",
    "Annotator.forward_with_embeddings = forward_with_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function encodes the images and passes them to the previous function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def forward(self: Annotator, generated_caption, images):\n",
    "        image_embeddings = self.encode_images(images)\n",
    "        output_sequence = self.forward_with_embeddings(generated_caption, image_embeddings)\n",
    "        return output_sequence\n",
    "Annotator.forward = forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best loss for learning a probability distribution is the cross-entropy loss, which is the negative log-likelihood of the true token.\n",
    "\n",
    "This loss is applied to each token and averaged over all the tokens. Also, we mask out the padding tokens so that they don't contribute to the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@staticmethod\n",
    "def loss_fn(generated_tokens: torch.Tensor, expected_output: torch.Tensor, lengths: torch.Tensor):\n",
    "    mask = torch.arange(generated_tokens.shape[1], device=generated_tokens.device)\\\n",
    "        .unsqueeze(0) < lengths.unsqueeze(1)\n",
    "    loss = nn.functional.cross_entropy(generated_tokens[mask], expected_output[mask], reduction=\"mean\")\n",
    "    return loss\n",
    "Annotator.loss_fn = loss_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the AdamW optimizer for its good performance in training transformers.\n",
    "\n",
    "To avoid gradient explosion, we used gradient clipping to clip the gradients to a maximum norm of 1.\n",
    "\n",
    "To avoid overfitting we used early stopping, which stops the training if the validation loss does not improve for a certain number of epochs.\n",
    "\n",
    "We also used learning rate scheduling, which reduces the learning rate by a certain factor if the validation loss does not improve for a certain number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(self, train: TextImageDataset, valid: TextImageDataset, epochs: int):\n",
    "    optim = torch.optim.Adam(self.parameters(), lr=5e-5)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=5, factor=0.5, threshold=0.01)\n",
    "    history = {\"train\": [], \"valid\": [], \"lr\": []}\n",
    "    bs = 8\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset=train, batch_size=bs, collate_fn=TextImageDataset.collate_fn,\n",
    "        sampler=torch.utils.data.RandomSampler(train, replacement=True, num_samples=bs * 100))\n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "        dataset=valid, batch_size=bs, collate_fn=TextImageDataset.collate_fn,\n",
    "        sampler=torch.utils.data.RandomSampler(valid, replacement=True, num_samples=bs * 50))\n",
    "\n",
    "    best_loss = float(\"inf\")\n",
    "    best_model = None\n",
    "    patience = 10\n",
    "    epochs_without_improvement = 0\n",
    "    threshold = 0.01\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        batches = tqdm(\n",
    "            train_loader,\n",
    "            desc=f\"Epoch {epoch + 1}/{epochs} (train)\")\n",
    "        self.train()\n",
    "        avg_loss = 0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        for i, (images, captions, lengths, _) in enumerate(batches):\n",
    "            images = images#.to(self.device)\n",
    "            captions = captions.to(self.device)\n",
    "            lengths = lengths.to(self.device)\n",
    "            output_sequence = captions[:, :-1]\n",
    "            expected_output = captions[:, 1:]\n",
    "            optim.zero_grad()\n",
    "            generated_tokens = self(output_sequence, images)\n",
    "            loss: torch.Tensor = self.loss_fn(generated_tokens, expected_output, lengths)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.parameters(), 1)\n",
    "            optim.step()\n",
    "            avg_loss += loss.item()\n",
    "            mask = torch.arange(generated_tokens.shape[1], device=generated_tokens.device)\\\n",
    "                .unsqueeze(0) < lengths.unsqueeze(1)\n",
    "            next_tokens = generated_tokens.argmax(dim=-1)\n",
    "            correct_predictions += (next_tokens[mask] == expected_output[mask]).sum().item()\n",
    "            total_predictions += next_tokens.numel()\n",
    "            batches.set_postfix(\n",
    "                loss=avg_loss / (i + 1), accuracy=f\"{correct_predictions / total_predictions:0.2%}\", lr=optim.param_groups[0][\"lr\"])\n",
    "        avg_loss /= i + 1\n",
    "        history[\"train\"].append(avg_loss)\n",
    "        batches = tqdm(\n",
    "            valid_loader,\n",
    "            desc=f\"Epoch {epoch + 1}/{epochs} (valid)\")\n",
    "        self.eval()\n",
    "        avg_loss = 0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        with torch.no_grad():\n",
    "            for i, (images, captions, lengths, _) in enumerate(batches):\n",
    "                images = images#.to(self.device)\n",
    "                captions = captions.to(self.device)\n",
    "                lengths = lengths.to(self.device)\n",
    "                output_sequence = captions[:, :-1]\n",
    "                expected_output = captions[:, 1:]\n",
    "                generated_tokens = self(output_sequence, images)\n",
    "                loss: torch.Tensor = self.loss_fn(generated_tokens, expected_output, lengths)\n",
    "                avg_loss += loss.item()\n",
    "                mask = torch.arange(generated_tokens.shape[1], device=generated_tokens.device)\\\n",
    "                    .unsqueeze(0) < lengths.unsqueeze(1)\n",
    "                next_tokens = generated_tokens.argmax(dim=-1)\n",
    "                correct_predictions += (next_tokens[mask] == expected_output[mask]).sum().item()\n",
    "                total_predictions += next_tokens.numel()\n",
    "                batches.set_postfix(\n",
    "                    loss=avg_loss / (i + 1), accuracy=f\"{correct_predictions / total_predictions:0.2%}\", lr=optim.param_groups[0][\"lr\"])\n",
    "        avg_loss /= i + 1\n",
    "        history[\"valid\"].append(avg_loss)\n",
    "        lr_scheduler.step(loss)\n",
    "        history[\"lr\"].append(optim.param_groups[0][\"lr\"])\n",
    "        if avg_loss < best_loss - threshold:\n",
    "            best_loss = avg_loss\n",
    "            best_model = self.state_dict()\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            if epochs_without_improvement >= patience:\n",
    "                break\n",
    "    self.load_state_dict(best_model)\n",
    "    return history\n",
    "Annotator.fit = fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a plot of the training history of the model. The training took approximately 1h on an NVIDIA 4070 SUPER.\n",
    "\n",
    "![training history](https://raw.githubusercontent.com/Etto48/CVProject/main/docs/history.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions are needed to save and load the model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(self: Annotator, path: str):\n",
    "    torch.save(self.state_dict(), path)\n",
    "Annotator.save = save\n",
    "\n",
    "@staticmethod\n",
    "def load(path: str):\n",
    "    annotator = Annotator()\n",
    "    annotator.load_state_dict(torch.load(\n",
    "        path, map_location=annotator.device, weights_only=True))\n",
    "    return annotator\n",
    "Annotator.load = load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the simplest algorithm to generate a caption for an image. At each step, we select the most probable token and append it to the caption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _greedy_search(self, image_embeddings: torch.Tensor, max_length: int, conditioning: Optional[str]):\n",
    "    batch_size = image_embeddings.shape[0]\n",
    "    if conditioning is not None:\n",
    "        conditioning = self.tokenizer.encode(conditioning)\n",
    "        captions = torch.full((batch_size, len(conditioning) + 1), self.tokenizer.max_token_value, dtype=torch.long, device=self.device)\n",
    "        captions[:, 1:] = torch.tensor(conditioning, device=self.device).view(1, -1)\n",
    "    else:\n",
    "        captions = torch.full((batch_size, 1), self.tokenizer.max_token_value, dtype=torch.long, device=self.device)\n",
    "    finished = torch.full((batch_size,), False, dtype=torch.bool, device=self.device)\n",
    "    if conditioning is not None:\n",
    "        lengths = torch.full((batch_size,), len(conditioning), dtype=torch.long, device=self.device)\n",
    "    else:\n",
    "        lengths = torch.full((batch_size,), 1, dtype=torch.long, device=self.device)\n",
    "    probabilities = torch.full((batch_size,), 1.0, dtype=torch.float32, device=self.device)\n",
    "    loading_bar = tqdm(total=max_length, desc=\"Generating captions (greedy)\")\n",
    "    while True:\n",
    "        generated_tokens_distribution = self.forward_with_embeddings(captions, image_embeddings)[:, -1]\n",
    "        generated_tokens = generated_tokens_distribution.argmax(dim=-1)\n",
    "        probabilities *= torch.softmax(generated_tokens_distribution, dim=-1)[torch.arange(batch_size), generated_tokens]\n",
    "        finished |= (generated_tokens == self.tokenizer.max_token_value)\n",
    "        if finished.all():\n",
    "            break\n",
    "        lengths += ~finished\n",
    "        captions = torch.cat([captions, generated_tokens.unsqueeze(1)], dim=1)\n",
    "        if max_length is not None and captions.shape[1] > max_length + 1:\n",
    "            break\n",
    "        loading_bar.update(1)\n",
    "    loading_bar.close()\n",
    "    return captions, lengths, probabilities, finished\n",
    "Annotator._greedy_search = _greedy_search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a bit more complex but still simple. At each step, we sample a token from the multinomial distribution of the predicted probabilities.\n",
    "Optionally, we can choose to sample from only the top-k most probable tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sample_search(self: Annotator, image_embeddings: torch.Tensor, max_length: int, top_k: Optional[int], conditioning: Optional[str]):\n",
    "    batch_size = image_embeddings.shape[0]\n",
    "    if conditioning is not None:\n",
    "        conditioning = self.tokenizer.encode(conditioning)\n",
    "        captions = torch.full((batch_size, len(conditioning) + 1), self.tokenizer.max_token_value, dtype=torch.long, device=self.device)\n",
    "        captions[:, 1:] = torch.tensor(conditioning, device=self.device).view(1, -1)\n",
    "    else:\n",
    "        captions = torch.full((batch_size, 1), self.tokenizer.max_token_value, dtype=torch.long, device=self.device)\n",
    "    finished = torch.full((batch_size,), False, dtype=torch.bool, device=self.device)\n",
    "    if conditioning is not None:\n",
    "        lengths = torch.full((batch_size,), len(conditioning), dtype=torch.long, device=self.device)\n",
    "    else:\n",
    "        lengths = torch.full((batch_size,), 1, dtype=torch.long, device=self.device)\n",
    "    probabilities = torch.full((batch_size,), 1.0, dtype=torch.float32, device=self.device)\n",
    "    loading_bar = tqdm(total=max_length, desc=\"Generating captions (sample)\")\n",
    "    while True:\n",
    "        generated_tokens_distribution = self.forward_with_embeddings(captions, image_embeddings)[:, -1]\n",
    "        match top_k:\n",
    "            case None:\n",
    "                generated_tokens = torch.multinomial(\n",
    "                    torch.softmax(generated_tokens_distribution, dim=-1), 1)\\\n",
    "                    .squeeze(1)\n",
    "            case _:\n",
    "                top_k_values, top_k_indices = torch.topk(\n",
    "                    generated_tokens_distribution, top_k, dim=-1)\n",
    "                top_k_probabilities = torch.softmax(top_k_values, dim=-1)\n",
    "                generated_tokens = torch.multinomial(\n",
    "                    top_k_probabilities, 1)\\\n",
    "                    .squeeze(1)\n",
    "                generated_tokens = top_k_indices[torch.arange(batch_size), generated_tokens]\n",
    "        probabilities *= torch.softmax(generated_tokens_distribution, dim=-1)[torch.arange(batch_size), generated_tokens]\n",
    "        finished |= (generated_tokens == self.tokenizer.max_token_value)\n",
    "        if finished.all():\n",
    "            break\n",
    "        lengths += ~finished\n",
    "        captions = torch.cat([captions, generated_tokens.unsqueeze(1)], dim=1)\n",
    "        if max_length is not None and captions.shape[1] > max_length + 1:\n",
    "            break\n",
    "        loading_bar.update(1)\n",
    "    loading_bar.close()\n",
    "    return captions, lengths, probabilities, finished\n",
    "Annotator._sample_search = _sample_search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the most complex of the three. We keep track of the top-k most probable captions at each step. When we generate the next token, we choose for each caption the top-k most probable sentences and discard the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _beam_search(self, image_embeddings: torch.Tensor, max_length: int, beam_size: int, conditioning: Optional[str]):\n",
    "    batch_size = image_embeddings.shape[0]\n",
    "    if conditioning is not None:\n",
    "        conditioning = self.tokenizer.encode(conditioning)\n",
    "        captions = torch.full((batch_size, beam_size, len(conditioning) + 1), self.tokenizer.max_token_value, dtype=torch.long, device=self.device)\n",
    "        captions[:, :, 1:] = torch.tensor(conditioning, device=self.device).view(1, 1, -1).repeat(batch_size, beam_size, 1)\n",
    "    else:\n",
    "        captions = torch.full((batch_size, beam_size, 1), self.tokenizer.max_token_value, dtype=torch.long, device=self.device)\n",
    "    finished = torch.full((batch_size, beam_size), False, dtype=torch.bool, device=self.device)\n",
    "    probabilities = torch.full((batch_size, beam_size), 1.0, dtype=torch.float32, device=self.device)\n",
    "    if conditioning is not None:\n",
    "        lengths = torch.full((batch_size, beam_size), len(conditioning), dtype=torch.long, device=self.device)\n",
    "    else:\n",
    "        lengths = torch.full((batch_size, beam_size), 1, dtype=torch.long, device=self.device)\n",
    "    loading_bar = tqdm(total=max_length, desc=\"Generating captions (beam)\")\n",
    "    first = True\n",
    "    while True:\n",
    "        if first:\n",
    "            first = False\n",
    "            generated_tokens_distribution = self.forward_with_embeddings(captions[:, 0, :], image_embeddings)[:, -1]\n",
    "            generated_tokens = torch.topk(generated_tokens_distribution, beam_size, dim=-1).indices\n",
    "            softmax = torch.softmax(generated_tokens_distribution, dim=-1)\n",
    "            probabilities = softmax[torch.arange(batch_size).view(batch_size, 1), generated_tokens]\n",
    "        else:\n",
    "            # batch and beam dimensions are collapsed into a single dimension for the forward pass\n",
    "            generated_tokens_distribution = self.forward_with_embeddings(\n",
    "                captions.view(batch_size * beam_size, -1), \n",
    "                image_embeddings\n",
    "                    .view(batch_size, 1, image_embeddings.shape[-2], image_embeddings.shape[-1])\n",
    "                    .repeat(1, beam_size, 1, 1)\n",
    "                    .view(batch_size * beam_size, image_embeddings.shape[-2], image_embeddings.shape[-1])\n",
    "                )\n",
    "            # take the last token distribution\n",
    "            generated_tokens_distribution = generated_tokens_distribution[:, -1]\n",
    "            # split back batch and beam dimensions\n",
    "            generated_tokens_distribution = generated_tokens_distribution.view(batch_size, beam_size, -1)\n",
    "            new_probabilities = probabilities.view(batch_size, beam_size, 1).repeat(1, 1, self.tokenizer.max_token_value + 1) * \\\n",
    "                torch.softmax(generated_tokens_distribution, dim=-1)\n",
    "            # collapse probabilities into a single dimension\n",
    "            new_probabilities = new_probabilities.view(batch_size, -1)\n",
    "            # select the top k most probable tokens among all beams\n",
    "            top_k_values, top_k_indices = torch.topk(new_probabilities, beam_size, dim=-1) # (B, beam_size * |V|)\n",
    "            # calculate the beam index and token index for each of the top k values\n",
    "            beam_indices = top_k_indices // (self.tokenizer.max_token_value + 1)\n",
    "            token_indices = top_k_indices % (self.tokenizer.max_token_value + 1)\n",
    "            # update the probabilities\n",
    "            old_probabilities = probabilities\n",
    "            probabilities = top_k_values\n",
    "            # update the captions\n",
    "            old_captions = captions\n",
    "            finished_mask = finished.view(batch_size, beam_size, 1).repeat(1, 1, captions.shape[2])\n",
    "            captions = captions[torch.arange(batch_size).view(batch_size, 1), beam_indices, :]\n",
    "            # restore the old captions and probabilities for finished beams that must not be updated\n",
    "            captions[finished_mask] = old_captions[finished_mask]\n",
    "            probabilities[finished] = old_probabilities[finished]\n",
    "            generated_tokens = token_indices\n",
    "        captions = torch.cat([captions, generated_tokens.view(batch_size, beam_size, 1)], dim=2)\n",
    "        finished = (captions[:, :, 1:] == self.tokenizer.max_token_value).any(dim=-1)\n",
    "        lengths = torch.full((batch_size, beam_size), captions.shape[2], dtype=torch.long, device=self.device)\n",
    "        for i in range(batch_size):\n",
    "            for j in range(beam_size):\n",
    "                if finished[i, j]:\n",
    "                    lengths[i, j] = (captions[i, j, 1:] == self.tokenizer.max_token_value).nonzero()[0].item() + 1\n",
    "        if finished.all():\n",
    "            break\n",
    "        if max_length is not None and captions.shape[2] > max_length + 1:\n",
    "            break\n",
    "        loading_bar.update(1)\n",
    "    loading_bar.close()\n",
    "    best_beam_indices = probabilities.argmax(dim=-1)\n",
    "    best_beam = captions[torch.arange(batch_size), best_beam_indices]\n",
    "    lengths = lengths[torch.arange(batch_size), best_beam_indices]\n",
    "    probabilities = probabilities[torch.arange(batch_size), best_beam_indices]\n",
    "    finished = finished[torch.arange(batch_size), best_beam_indices]\n",
    "\n",
    "    return best_beam, lengths, probabilities, finished\n",
    "Annotator._beam_search = _beam_search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, this is the function that calls one of the three generation algorithms and returns the generated caption for each input image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate(self: Annotator, images, max_length: int = 15, mode: Literal[\"greedy\", \"sample\", \"beam\"] = \"greedy\", top_k: Optional[int] = None, conditioning: Optional[str] = None):\n",
    "    self.eval()\n",
    "    batched = isinstance(images, list) or isinstance(images, tuple)\n",
    "    with torch.no_grad():\n",
    "        images = images\n",
    "        images = [images] if not batched else images\n",
    "        image_embeddings = self.encode_images(images)\n",
    "\n",
    "        match mode:\n",
    "            case \"greedy\":\n",
    "                captions, lengths, probabilities, finished = self._greedy_search(image_embeddings, max_length, conditioning)\n",
    "            case \"sample\":\n",
    "                captions, lengths, probabilities, finished = self._sample_search(image_embeddings, max_length, top_k, conditioning)\n",
    "            case \"beam\":\n",
    "                captions, lengths, probabilities, finished = self._beam_search(image_embeddings, max_length, top_k, conditioning)\n",
    "            \n",
    "    outputs = []\n",
    "    for i in range(captions.shape[0]):\n",
    "        caption = captions[i, 1:lengths[i]].cpu().numpy()\n",
    "        output = self.tokenizer.decode(caption)\n",
    "        if not finished[i]:\n",
    "            output += \"...\"\n",
    "        outputs.append(output)\n",
    "    if batched:\n",
    "        return outputs, probabilities\n",
    "    else:\n",
    "        assert len(outputs) == 1\n",
    "        return outputs[0], probabilities[0]\n",
    "Annotator.annotate = annotate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see the model in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotator = Annotator.from_pretrained()\n",
    "\n",
    "l = 3\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    dataset=valid, batch_size=l * l, collate_fn=TextImageDataset.collate_fn,\n",
    "    sampler=torch.utils.data.RandomSampler(valid, replacement=True, num_samples=l * l))\n",
    "images, _, _, _ = next(iter(valid_loader))\n",
    "captions_beam, beam_prob = annotator.annotate(images, mode=\"beam\", top_k=10)\n",
    "captions_greedy, greedy_prob = annotator.annotate(images, mode=\"greedy\")\n",
    "captions_sample, sample_prob = annotator.annotate(images, mode=\"sample\", top_k=10)\n",
    "plt.figure(figsize=(22, 15))\n",
    "for i in range(l * l):\n",
    "    plt.subplot(l, l, i + 1)\n",
    "    plt.imshow(images[i].permute(1, 2, 0))\n",
    "    plt.title(f\"Beam: {captions_beam[i]} ({beam_prob[i]:.3%})\\n\"\n",
    "                f\"Greedy: {captions_greedy[i]} ({greedy_prob[i]:.3%})\\n\"\n",
    "                f\"Sample: {captions_sample[i]} ({sample_prob[i]:.3%})\")\n",
    "    plt.axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
